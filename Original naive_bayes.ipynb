{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3zJrHl2YYkg",
    "outputId": "be8a664b-8191-4078-c880-02a23eaf81cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from google.colab import drive\\ndrive.mount(\"/content/gdrive\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iibLSvrIa4nu"
   },
   "source": [
    "# **Prepare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cW6UFHEBS_1G",
    "outputId": "c39a9026-421e-4360-a050-f7e67883e7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/transformers-3.0.2+computecanada-py3-none-any.whl\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (2022.1.18+computecanada)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Using cached tokenizers-0.8.1rc1.tar.gz (97 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (0.1.97)\n",
      "Requirement already satisfied: numpy in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (1.23.0+computecanada)\n",
      "Requirement already satisfied: packaging in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (21.3+computecanada)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sacremoses-0.0.49+computecanada-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (3.8.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (4.64.1+computecanada)\n",
      "Requirement already satisfied: requests in ./jp1/lib/python3.10/site-packages (from transformers==3.0.2) (2.28.1+computecanada)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./jp1/lib/python3.10/site-packages (from packaging->transformers==3.0.2) (3.0.9+computecanada)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jp1/lib/python3.10/site-packages (from requests->transformers==3.0.2) (2022.9.24+computecanada)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jp1/lib/python3.10/site-packages (from requests->transformers==3.0.2) (3.4+computecanada)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./jp1/lib/python3.10/site-packages (from requests->transformers==3.0.2) (2.1.1+computecanada)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./jp1/lib/python3.10/site-packages (from requests->transformers==3.0.2) (1.26.13+computecanada)\n",
      "Requirement already satisfied: click in ./jp1/lib/python3.10/site-packages (from sacremoses->transformers==3.0.2) (8.1.3+computecanada)\n",
      "Requirement already satisfied: six in ./jp1/lib/python3.10/site-packages (from sacremoses->transformers==3.0.2) (1.16.0+computecanada)\n",
      "Requirement already satisfied: joblib in ./jp1/lib/python3.10/site-packages (from sacremoses->transformers==3.0.2) (1.2.0+computecanada)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[48 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-tsi5kic4/overlay/lib/python3.10/site-packages/setuptools/dist.py:534: UserWarning: Normalizing '0.8.1.rc1' to '0.8.1rc1'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(tmpl.format(**locals()))\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/transformers-2.9.0+computecanada-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sacremoses-0.0.49+computecanada-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in ./jp1/lib/python3.10/site-packages (from transformers==2.9) (3.8.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./jp1/lib/python3.10/site-packages (from transformers==2.9) (4.64.1+computecanada)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Using cached tokenizers-0.7.0.tar.gz (81 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in ./jp1/lib/python3.10/site-packages (from transformers==2.9) (2022.1.18+computecanada)\n",
      "Requirement already satisfied: sentencepiece in ./jp1/lib/python3.10/site-packages (from transformers==2.9) (0.1.97)\n",
      "Requirement already satisfied: numpy in ./jp1/lib/python3.10/site-packages (from transformers==2.9) (1.23.0+computecanada)\n",
      "Requirement already satisfied: requests in ./jp1/lib/python3.10/site-packages (from transformers==2.9) (2.28.1+computecanada)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./jp1/lib/python3.10/site-packages (from requests->transformers==2.9) (2.1.1+computecanada)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jp1/lib/python3.10/site-packages (from requests->transformers==2.9) (3.4+computecanada)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./jp1/lib/python3.10/site-packages (from requests->transformers==2.9) (1.26.13+computecanada)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jp1/lib/python3.10/site-packages (from requests->transformers==2.9) (2022.9.24+computecanada)\n",
      "Requirement already satisfied: joblib in ./jp1/lib/python3.10/site-packages (from sacremoses->transformers==2.9) (1.2.0+computecanada)\n",
      "Requirement already satisfied: six in ./jp1/lib/python3.10/site-packages (from sacremoses->transformers==2.9) (1.16.0+computecanada)\n",
      "Requirement already satisfied: click in ./jp1/lib/python3.10/site-packages (from sacremoses->transformers==2.9) (8.1.3+computecanada)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[46 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: torch in ./jp1/lib/python3.10/site-packages (1.13.1+computecanada)\n",
      "Requirement already satisfied: torchvision in ./jp1/lib/python3.10/site-packages (0.14.1+cu116)\n",
      "Requirement already satisfied: torchaudio in ./jp1/lib/python3.10/site-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in ./jp1/lib/python3.10/site-packages (from torch) (4.4.0+computecanada)\n",
      "Requirement already satisfied: requests in ./jp1/lib/python3.10/site-packages (from torchvision) (2.28.1+computecanada)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./jp1/lib/python3.10/site-packages (from torchvision) (9.2.0+computecanada)\n",
      "Requirement already satisfied: numpy in ./jp1/lib/python3.10/site-packages (from torchvision) (1.23.0+computecanada)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jp1/lib/python3.10/site-packages (from requests->torchvision) (2022.9.24+computecanada)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./jp1/lib/python3.10/site-packages (from requests->torchvision) (1.26.13+computecanada)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./jp1/lib/python3.10/site-packages (from requests->torchvision) (2.1.1+computecanada)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jp1/lib/python3.10/site-packages (from requests->torchvision) (3.4+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: nltk in ./jp1/lib/python3.10/site-packages (3.8)\n",
      "Requirement already satisfied: tqdm in ./jp1/lib/python3.10/site-packages (from nltk) (4.64.1+computecanada)\n",
      "Requirement already satisfied: click in ./jp1/lib/python3.10/site-packages (from nltk) (8.1.3+computecanada)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./jp1/lib/python3.10/site-packages (from nltk) (2022.1.18+computecanada)\n",
      "Requirement already satisfied: joblib in ./jp1/lib/python3.10/site-packages (from nltk) (1.2.0+computecanada)\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: Keras-Preprocessing in ./jp1/lib/python3.10/site-packages (1.1.2+computecanada)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./jp1/lib/python3.10/site-packages (from Keras-Preprocessing) (1.23.0+computecanada)\n",
      "Requirement already satisfied: six>=1.9.0 in ./jp1/lib/python3.10/site-packages (from Keras-Preprocessing) (1.16.0+computecanada)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.0.2\n",
    "!pip install transformers==2.9\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install nltk\n",
    "!pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UbyLQeCxajSS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6001568/zhe8nov1/jp1/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-23 21:17:33.158668: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-23 21:17:37.597677: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-23 21:18:21.083243: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 21:18:21.085937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 21:18:21.085978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DistilBertTokenizer, RobertaTokenizer\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from transformers import TFXLNetModel, XLNetTokenizer\n",
    "import nltk\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tOwbMmf4al9k"
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HU7L7PJ_agG2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.scruples-anecdotes.csv\" ,skip_blank_lines=True)\n",
    "\n",
    "dev =pd.read_csv(\"dev-scruples-anecdotes.csv\", skip_blank_lines=True)\n",
    "\n",
    "test =pd.read_csv(\"test.scruples-anecdotes.csv\", skip_blank_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train['text'].isnull().sum())\n",
    "print(dev['text'].isnull().sum())\n",
    "print(test['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "dev = dev.dropna()\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train['text'].isnull().sum())\n",
    "print(dev['text'].isnull().sum())\n",
    "print(test['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gMl9l3G_rqt8"
   },
   "outputs": [],
   "source": [
    "#train_50Percent = train.sample(frac=0.50, random_state=42)\n",
    "train_df = train[['text','binarized_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HYm_AX236DZ0"
   },
   "outputs": [],
   "source": [
    "dev_df = dev[['text','binarized_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L637VhuITUWb"
   },
   "outputs": [],
   "source": [
    "test_df =  test[['text','binarized_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GM3SsTtyvBjo",
    "outputId": "9ee46a45-2f94-4370-f98a-3533135b67c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My boyfriend (well just call him M) and I are ...</td>\n",
       "      <td>RIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMy really good friend is getting married....</td>\n",
       "      <td>RIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been very lonely for a long time, few fri...</td>\n",
       "      <td>RIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alt account because friends know my real one. ...</td>\n",
       "      <td>RIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pretty much was hanging out with two friends. ...</td>\n",
       "      <td>WRONG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>For reference we're both juniors at a T20 coll...</td>\n",
       "      <td>WRONG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>So this just happened and I'm a little taken a...</td>\n",
       "      <td>WRONG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>So, I've been friends with someone for a few y...</td>\n",
       "      <td>RIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>My first post!\\n\\nNo words were exchanged in t...</td>\n",
       "      <td>RIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Background info: I’m in college and my dorm is...</td>\n",
       "      <td>WRONG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2343 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text binarized_label\n",
       "0     My boyfriend (well just call him M) and I are ...           RIGHT\n",
       "2      \\n\\nMy really good friend is getting married....           RIGHT\n",
       "3     I've been very lonely for a long time, few fri...           RIGHT\n",
       "4     Alt account because friends know my real one. ...           RIGHT\n",
       "5     Pretty much was hanging out with two friends. ...           WRONG\n",
       "...                                                 ...             ...\n",
       "2494  For reference we're both juniors at a T20 coll...           WRONG\n",
       "2495  So this just happened and I'm a little taken a...           WRONG\n",
       "2497  So, I've been friends with someone for a few y...           RIGHT\n",
       "2498  My first post!\\n\\nNo words were exchanged in t...           RIGHT\n",
       "2499  Background info: I’m in college and my dorm is...           WRONG\n",
       "\n",
       "[2343 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Bkb7_CaVuS6G"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train_df['num_binarized_label']= label_encoder.fit_transform(train_df['binarized_label'])\n",
    "dev_df['num_binarized_label']= label_encoder.fit_transform(dev_df['binarized_label'])\n",
    "test_df['num_binarized_label']= label_encoder.fit_transform(test_df['binarized_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cyfZWryt3Lt4",
    "outputId": "3e78dd49-b39a-443d-95f0-9c95f8c5249d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>num_binarized_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27761</th>\n",
       "      <td>So a bit of background info. My girlfriend has...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27762</th>\n",
       "      <td>Context:\\n\\nI’ve been best friends with this g...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27763</th>\n",
       "      <td>So me (19M) and my gf’s (18F) relationship has...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27764</th>\n",
       "      <td>A little info, I’m an Early College student, f...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27765</th>\n",
       "      <td>I was on the way home on a long 2 lane bridge ...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26196 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text binarized_label  \\\n",
       "0      Backstory: So, I got an Xbox one for Christmas...           RIGHT   \n",
       "1      I work with about six other people at might jo...           RIGHT   \n",
       "2      Context: There was an Instagram post about unp...           RIGHT   \n",
       "3      Me and my friends spent sometime organizing a ...           WRONG   \n",
       "4      A little background. I'm a far from rich guy w...           RIGHT   \n",
       "...                                                  ...             ...   \n",
       "27761  So a bit of background info. My girlfriend has...           RIGHT   \n",
       "27762  Context:\\n\\nI’ve been best friends with this g...           RIGHT   \n",
       "27763  So me (19M) and my gf’s (18F) relationship has...           RIGHT   \n",
       "27764  A little info, I’m an Early College student, f...           RIGHT   \n",
       "27765  I was on the way home on a long 2 lane bridge ...           RIGHT   \n",
       "\n",
       "       num_binarized_label  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        1  \n",
       "4                        0  \n",
       "...                    ...  \n",
       "27761                    0  \n",
       "27762                    0  \n",
       "27763                    0  \n",
       "27764                    0  \n",
       "27765                    0  \n",
       "\n",
       "[26196 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5c18o1K2bo01"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove emojis and special chars\n",
    "    clean=text\n",
    "    #reg = re.compile('\\\\.+?(?=\\B|$)')\n",
    "    #clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n",
    "    #reg = re.compile('\\x89Û_')\n",
    "    #clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
    "    reg = re.compile('\\&amp')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n",
    "    reg = re.compile('\\\\n')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
    "    \n",
    "    #Remove hashtag symbol (#)\n",
    "    #clean = clean.apply(lambda r: r.replace('#', ''))\n",
    "\n",
    "    #Remove user names\n",
    "    reg = re.compile('@[a-zA-Z0-9\\_]+')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n",
    "\n",
    "    #Remove URLs\n",
    "    reg = re.compile('https?\\S+(?=\\s|$)')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='www'))\n",
    "\n",
    "    #Lowercase\n",
    "    #clean = clean.apply(lambda r: r.lower())\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S2FiZCn8nH1P"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.replace(np.nan, '', regex=True)\n",
    "\n",
    "dev_df = dev_df.replace(np.nan, '', regex=True)\n",
    "\n",
    "test_df = test_df.replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "soOLE6zFbqiP"
   },
   "outputs": [],
   "source": [
    "train_df['clean'] = clean_text(train_df['text'])\n",
    "dev_df['clean'] = clean_text(dev_df['text'])\n",
    "test_df['clean'] = clean_text(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MgvH0C9-ncI6",
    "outputId": "e1234768-98cd-4580-ccec-8eb918bc733c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>num_binarized_label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27761</th>\n",
       "      <td>So a bit of background info. My girlfriend has...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>So a bit of background info. My girlfriend has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27762</th>\n",
       "      <td>Context:\\n\\nI’ve been best friends with this g...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Context:  I’ve been best friends with this guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27763</th>\n",
       "      <td>So me (19M) and my gf’s (18F) relationship has...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>So me (19M) and my gf’s (18F) relationship has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27764</th>\n",
       "      <td>A little info, I’m an Early College student, f...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>A little info, I’m an Early College student, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27765</th>\n",
       "      <td>I was on the way home on a long 2 lane bridge ...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>I was on the way home on a long 2 lane bridge ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26196 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text binarized_label  \\\n",
       "0      Backstory: So, I got an Xbox one for Christmas...           RIGHT   \n",
       "1      I work with about six other people at might jo...           RIGHT   \n",
       "2      Context: There was an Instagram post about unp...           RIGHT   \n",
       "3      Me and my friends spent sometime organizing a ...           WRONG   \n",
       "4      A little background. I'm a far from rich guy w...           RIGHT   \n",
       "...                                                  ...             ...   \n",
       "27761  So a bit of background info. My girlfriend has...           RIGHT   \n",
       "27762  Context:\\n\\nI’ve been best friends with this g...           RIGHT   \n",
       "27763  So me (19M) and my gf’s (18F) relationship has...           RIGHT   \n",
       "27764  A little info, I’m an Early College student, f...           RIGHT   \n",
       "27765  I was on the way home on a long 2 lane bridge ...           RIGHT   \n",
       "\n",
       "       num_binarized_label                                              clean  \n",
       "0                        0  Backstory: So, I got an Xbox one for Christmas...  \n",
       "1                        0  I work with about six other people at might jo...  \n",
       "2                        0  Context: There was an Instagram post about unp...  \n",
       "3                        1  Me and my friends spent sometime organizing a ...  \n",
       "4                        0  A little background. I'm a far from rich guy w...  \n",
       "...                    ...                                                ...  \n",
       "27761                    0  So a bit of background info. My girlfriend has...  \n",
       "27762                    0  Context:  I’ve been best friends with this guy...  \n",
       "27763                    0  So me (19M) and my gf’s (18F) relationship has...  \n",
       "27764                    0  A little info, I’m an Early College student, f...  \n",
       "27765                    0  I was on the way home on a long 2 lane bridge ...  \n",
       "\n",
       "[26196 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b1omAf1cd6Nh",
    "outputId": "0858e446-4ce4-4622-83c8-4dff803ac5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   Throwaway for obvious reasons.     I dropped out of uni 2 years ago because I was planning on moving to a different country and going to Uni there. However, my parents didn't want to help me in any way and I couldn't afford it myself. They've always supported my younger sister though, bought her an apartment and they're even paying her bills, even though she earns enough money to do that herself. So atm I'm stuck at home with them, which sucks as I am 25 yo and I feel like I'm wasting my life completely.     Both my parents were extremely abusive, especially my dad. As a result, I have social anxiety, trust issues, panic attacks etc. I can deal with everything as I got used to it anyway, but lately things have gotten worse.     A few years ago my dad was diagnosed with type II diabetes, and he retired. He's been staying at home ever since and he's slowly but surely becoming a vegetable. He had a stroke which left him unable to speak/walk properly. I'm no doctor, but I found his hospital records and even talked to a nurse, and she said he's a bloody mess and he could die at any time. The worst part is, he doesn't realise what's happening around him anymore. He leaves the apartment door open at night, he forgets stuff on the stove, and he makes a huge mess everywhere. I talked to my mum about this, I don't feel safe with him around. He clearly lost his mind but she refuses to acknowledge it. Throughout the years, I've tried to tell her there's something really wrong with him, but she always just shrugged it off. He was an awful husband, so she doesn't love him, but she says she feels pity for him.     I suggested we put him in a retirement home, as he clearly cannot take care of himself and he's putting our lives in danger. I sleep with ear plugs because he slams doors/drops things all the time. I'm afraid he might set our apartment on fire or worse. My mum doesn't want to do that because she's afraid of his reaction; she thinks he's going to get a heart attack if she tries to bring it up. He's always hated hospitals and he always refused treatment, even when he got bitten by a stray dog, even after his stroke, so I see where my mum is coming from. However, I've had enough.     Yesterday I heard him choke, he was struggling to breathe and honestly, it sounded like he was gonna die. And I was relieved. My first instinct wasn't to help him, instead I kept doing my skincare, honestly hoping he'd just die already. He didn't. I told my mum what happened when she got home and she thinks I'm a monster. But am I really? This person has abused me my entire life, has beaten the living shit out of me, hell he has even tried to kill me a few times. I know my life would get better if he died, my mum would sell our apartment and I'd finally have money to go to college and start my own life. So AITA for wanting my abusive father gone?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SlXUh6eaouY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6inA2y46ayU3"
   },
   "source": [
    "# **Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "Ta4HcJDXi80-",
    "outputId": "ef441029-e0fa-496a-c9fe-8e7cfcc7c682"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>num_binarized_label</th>\n",
       "      <th>clean</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "      <td>backstory: so, i got an xbox one for christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "      <td>i work with about six other people at might jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "      <td>context: there was an instagram post about unp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "      <td>me and my friends spent sometime organizing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "      <td>a little background. i'm a far from rich guy w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text binarized_label  \\\n",
       "0  Backstory: So, I got an Xbox one for Christmas...           RIGHT   \n",
       "1  I work with about six other people at might jo...           RIGHT   \n",
       "2  Context: There was an Instagram post about unp...           RIGHT   \n",
       "3  Me and my friends spent sometime organizing a ...           WRONG   \n",
       "4  A little background. I'm a far from rich guy w...           RIGHT   \n",
       "\n",
       "   num_binarized_label                                              clean  \\\n",
       "0                    0  Backstory: So, I got an Xbox one for Christmas...   \n",
       "1                    0  I work with about six other people at might jo...   \n",
       "2                    0  Context: There was an Instagram post about unp...   \n",
       "3                    1  Me and my friends spent sometime organizing a ...   \n",
       "4                    0  A little background. I'm a far from rich guy w...   \n",
       "\n",
       "                                               lower  \n",
       "0  backstory: so, i got an xbox one for christmas...  \n",
       "1  i work with about six other people at might jo...  \n",
       "2  context: there was an instagram post about unp...  \n",
       "3  me and my friends spent sometime organizing a ...  \n",
       "4  a little background. i'm a far from rich guy w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"lower\"] = train_df[\"clean\"].apply(lambda x: x.lower())\n",
    "\n",
    "dev_df[\"lower\"] = dev_df[\"clean\"].apply(lambda x: x.lower())\n",
    "\n",
    "test_df[\"lower\"] = test_df[\"clean\"].apply(lambda x: x.lower())\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8a01ZoVi8yh",
    "outputId": "5d6da340-8170-436c-bba3-c7b6a9d439c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Requirement already satisfied: contractions in ./jp1/lib/python3.10/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./jp1/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in ./jp1/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (1.4.4+computecanada)\n",
      "Requirement already satisfied: anyascii in ./jp1/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import contractions\n",
    "\n",
    "train_df[\"lower\"] = train_df[\"lower\"].apply(lambda x: contractions.fix(x))\n",
    "dev_df[\"lower\"] = dev_df[\"lower\"].apply(lambda x: contractions.fix(x))\n",
    "test_df[\"lower\"] = test_df[\"lower\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nJHpsc2homtd"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "        Remove non-ASCII characters \n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) \n",
    "\n",
    "# remove non-ascii characters from the text\n",
    "\n",
    "train_df[\"lower\"] = train_df[\"lower\"].apply(lambda x: remove_non_ascii(x))\n",
    "dev_df[\"lower\"] = dev_df[\"lower\"].apply(lambda x: remove_non_ascii(x))\n",
    "test_df[\"lower\"] = test_df[\"lower\"].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Jv6RO3iKop6o"
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "        Remove special special characters, including symbols, emojis, and other graphic characters\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "# remove non-ascii characters from the text\n",
    "\n",
    "train_df[\"lower\"] = train_df[\"lower\"].apply(lambda x: remove_special_characters(x))\n",
    "dev_df[\"lower\"] = dev_df[\"lower\"].apply(lambda x: remove_special_characters(x))\n",
    "test_df[\"lower\"] = test_df[\"lower\"].apply(lambda x: remove_special_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caTZ_mtZEmIZ",
    "outputId": "e63a67c6-40d3-4e0f-84d9-1660d7138643"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhe8nov1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5EwtF6_kEFKs"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the tweet base texts.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['tokenized'] = train_df['lower'].apply(word_tokenize)\n",
    "dev_df['tokenized'] = dev_df['lower'].apply(word_tokenize)\n",
    "test_df['tokenized'] = test_df['lower'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ASAwOzNfEFFq",
    "outputId": "ef9941a8-5eba-498d-dcae-4073d1fd3e7b"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def snowball_stemmer(text):\n",
    "    \"\"\"\n",
    "        Stem words in list of tokenized words with SnowballStemmer\n",
    "    \"\"\"\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcsmJA_aGdnj",
    "outputId": "1f132f39-7a15-4aa8-89d1-31488b56931d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/zhe8nov1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/zhe8nov1/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bSfB3aFyE7Y8"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    \"\"\"\n",
    "        Lemmatize the tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['lemmatize_word_wo_pos'] = train_df['tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "#train_df['lemmatize_word_wo_pos'] = train_df['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])\n",
    "\n",
    "dev_df['lemmatize_word_wo_pos'] = dev_df['tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "#dev_df['lemmatize_word_wo_pos'] = dev_df['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])\n",
    "\n",
    "test_df['lemmatize_word_wo_pos'] = test_df['tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "#test_df['lemmatize_word_wo_pos'] = test_df['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-y8ZnATKE7O9"
   },
   "outputs": [],
   "source": [
    "connect1 = []\n",
    "for i in train_df['lemmatize_word_wo_pos']:\n",
    "  connect1.append(' '.join(i))\n",
    "train_df['clean1'] = connect1\n",
    "\n",
    "\n",
    "connect2 = []\n",
    "for i in dev_df['lemmatize_word_wo_pos']:\n",
    "  connect2.append(' '.join(i))\n",
    "dev_df['clean1'] = connect2\n",
    "\n",
    "\n",
    "connect3 = []\n",
    "for i in test_df['lemmatize_word_wo_pos']:\n",
    "  connect3.append(' '.join(i))\n",
    "test_df['clean1'] = connect3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hzhA3WzaKylJ",
    "outputId": "37a3e34f-48c1-4ad9-b7b9-09e300dd94ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My boyfriend (well just call him M) and I are creators on Instagram (he is also on TikTok). I\\'m a digital and traditional artist and in one of my classes we create t shirts and things (socks, lanyards, water bottles, stickers, you name it). I\\'m a big fan of putting my art on t shirts so I can sell them.  M wanted to make some posts for TikTok and wanted to use the shirt I made today. My one personal rule is that if the shirt isn\\'t being given to someone else, then I wear it first (this is because I know I wont get it back. Ive made one other shirt that I\\'ve worn, and he wore it for a week and hasn\\'t given it back.) So I told him no, because I haven\\'t worn the shirt yet.   M proceeded to complain about not having content to post, and how his IG post today didn\\'t get any engagement, saying \"of course you dont know, because you never look.\"  Am I the asshole for wanting to wear the shirt I made just once?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df['clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JOt-23ZRE7Tx",
    "outputId": "73e0807f-eda6-480e-ee81-7d36f289ff35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>num_binarized_label</th>\n",
       "      <th>clean</th>\n",
       "      <th>lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>clean1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My boyfriend (well just call him M) and I are ...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>My boyfriend (well just call him M) and I are ...</td>\n",
       "      <td>my boyfriend (well just call him m) and i are ...</td>\n",
       "      <td>[my, boyfriend, (, well, just, call, him, m, )...</td>\n",
       "      <td>[my, boyfriend, (, well, just, call, him, m, )...</td>\n",
       "      <td>my boyfriend ( well just call him m ) and i ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMy really good friend is getting married....</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>My really good friend is getting married. Y...</td>\n",
       "      <td>my really good friend is getting married. y...</td>\n",
       "      <td>[my, really, good, friend, is, getting, marrie...</td>\n",
       "      <td>[my, really, good, friend, is, getting, marrie...</td>\n",
       "      <td>my really good friend is getting married . yay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been very lonely for a long time, few fri...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>I've been very lonely for a long time, few fri...</td>\n",
       "      <td>i have been very lonely for a long time, few f...</td>\n",
       "      <td>[i, have, been, very, lonely, for, a, long, ti...</td>\n",
       "      <td>[i, have, been, very, lonely, for, a, long, ti...</td>\n",
       "      <td>i have been very lonely for a long time , few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alt account because friends know my real one. ...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Alt account because friends know my real one. ...</td>\n",
       "      <td>alt account because friends know my real one. ...</td>\n",
       "      <td>[alt, account, because, friends, know, my, rea...</td>\n",
       "      <td>[alt, account, because, friend, know, my, real...</td>\n",
       "      <td>alt account because friend know my real one . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pretty much was hanging out with two friends. ...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>Pretty much was hanging out with two friends. ...</td>\n",
       "      <td>pretty much was hanging out with two friends. ...</td>\n",
       "      <td>[pretty, much, was, hanging, out, with, two, f...</td>\n",
       "      <td>[pretty, much, wa, hanging, out, with, two, fr...</td>\n",
       "      <td>pretty much wa hanging out with two friend . i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>For reference we're both juniors at a T20 coll...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>For reference we're both juniors at a T20 coll...</td>\n",
       "      <td>for reference we are both juniors at a t20 col...</td>\n",
       "      <td>[for, reference, we, are, both, juniors, at, a...</td>\n",
       "      <td>[for, reference, we, are, both, junior, at, a,...</td>\n",
       "      <td>for reference we are both junior at a t20 coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>So this just happened and I'm a little taken a...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>So this just happened and I'm a little taken a...</td>\n",
       "      <td>so this just happened and i am a little taken ...</td>\n",
       "      <td>[so, this, just, happened, and, i, am, a, litt...</td>\n",
       "      <td>[so, this, just, happened, and, i, am, a, litt...</td>\n",
       "      <td>so this just happened and i am a little taken ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>So, I've been friends with someone for a few y...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>So, I've been friends with someone for a few y...</td>\n",
       "      <td>so, i have been friends with someone for a few...</td>\n",
       "      <td>[so, ,, i, have, been, friends, with, someone,...</td>\n",
       "      <td>[so, ,, i, have, been, friend, with, someone, ...</td>\n",
       "      <td>so , i have been friend with someone for a few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>My first post!\\n\\nNo words were exchanged in t...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>My first post!  No words were exchanged in thi...</td>\n",
       "      <td>my first post!  no words were exchanged in thi...</td>\n",
       "      <td>[my, first, post, !, no, words, were, exchange...</td>\n",
       "      <td>[my, first, post, !, no, word, were, exchanged...</td>\n",
       "      <td>my first post ! no word were exchanged in this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Background info: I’m in college and my dorm is...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>Background info: I’m in college and my dorm is...</td>\n",
       "      <td>background info: i am in college and my dorm i...</td>\n",
       "      <td>[background, info, :, i, am, in, college, and,...</td>\n",
       "      <td>[background, info, :, i, am, in, college, and,...</td>\n",
       "      <td>background info : i am in college and my dorm ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2343 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text binarized_label  \\\n",
       "0     My boyfriend (well just call him M) and I are ...           RIGHT   \n",
       "2      \\n\\nMy really good friend is getting married....           RIGHT   \n",
       "3     I've been very lonely for a long time, few fri...           RIGHT   \n",
       "4     Alt account because friends know my real one. ...           RIGHT   \n",
       "5     Pretty much was hanging out with two friends. ...           WRONG   \n",
       "...                                                 ...             ...   \n",
       "2494  For reference we're both juniors at a T20 coll...           WRONG   \n",
       "2495  So this just happened and I'm a little taken a...           WRONG   \n",
       "2497  So, I've been friends with someone for a few y...           RIGHT   \n",
       "2498  My first post!\\n\\nNo words were exchanged in t...           RIGHT   \n",
       "2499  Background info: I’m in college and my dorm is...           WRONG   \n",
       "\n",
       "      num_binarized_label                                              clean  \\\n",
       "0                       0  My boyfriend (well just call him M) and I are ...   \n",
       "2                       0     My really good friend is getting married. Y...   \n",
       "3                       0  I've been very lonely for a long time, few fri...   \n",
       "4                       0  Alt account because friends know my real one. ...   \n",
       "5                       1  Pretty much was hanging out with two friends. ...   \n",
       "...                   ...                                                ...   \n",
       "2494                    1  For reference we're both juniors at a T20 coll...   \n",
       "2495                    1  So this just happened and I'm a little taken a...   \n",
       "2497                    0  So, I've been friends with someone for a few y...   \n",
       "2498                    0  My first post!  No words were exchanged in thi...   \n",
       "2499                    1  Background info: I’m in college and my dorm is...   \n",
       "\n",
       "                                                  lower  \\\n",
       "0     my boyfriend (well just call him m) and i are ...   \n",
       "2        my really good friend is getting married. y...   \n",
       "3     i have been very lonely for a long time, few f...   \n",
       "4     alt account because friends know my real one. ...   \n",
       "5     pretty much was hanging out with two friends. ...   \n",
       "...                                                 ...   \n",
       "2494  for reference we are both juniors at a t20 col...   \n",
       "2495  so this just happened and i am a little taken ...   \n",
       "2497  so, i have been friends with someone for a few...   \n",
       "2498  my first post!  no words were exchanged in thi...   \n",
       "2499  background info: i am in college and my dorm i...   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     [my, boyfriend, (, well, just, call, him, m, )...   \n",
       "2     [my, really, good, friend, is, getting, marrie...   \n",
       "3     [i, have, been, very, lonely, for, a, long, ti...   \n",
       "4     [alt, account, because, friends, know, my, rea...   \n",
       "5     [pretty, much, was, hanging, out, with, two, f...   \n",
       "...                                                 ...   \n",
       "2494  [for, reference, we, are, both, juniors, at, a...   \n",
       "2495  [so, this, just, happened, and, i, am, a, litt...   \n",
       "2497  [so, ,, i, have, been, friends, with, someone,...   \n",
       "2498  [my, first, post, !, no, words, were, exchange...   \n",
       "2499  [background, info, :, i, am, in, college, and,...   \n",
       "\n",
       "                                  lemmatize_word_wo_pos  \\\n",
       "0     [my, boyfriend, (, well, just, call, him, m, )...   \n",
       "2     [my, really, good, friend, is, getting, marrie...   \n",
       "3     [i, have, been, very, lonely, for, a, long, ti...   \n",
       "4     [alt, account, because, friend, know, my, real...   \n",
       "5     [pretty, much, wa, hanging, out, with, two, fr...   \n",
       "...                                                 ...   \n",
       "2494  [for, reference, we, are, both, junior, at, a,...   \n",
       "2495  [so, this, just, happened, and, i, am, a, litt...   \n",
       "2497  [so, ,, i, have, been, friend, with, someone, ...   \n",
       "2498  [my, first, post, !, no, word, were, exchanged...   \n",
       "2499  [background, info, :, i, am, in, college, and,...   \n",
       "\n",
       "                                                 clean1  \n",
       "0     my boyfriend ( well just call him m ) and i ar...  \n",
       "2     my really good friend is getting married . yay...  \n",
       "3     i have been very lonely for a long time , few ...  \n",
       "4     alt account because friend know my real one . ...  \n",
       "5     pretty much wa hanging out with two friend . i...  \n",
       "...                                                 ...  \n",
       "2494  for reference we are both junior at a t20 coll...  \n",
       "2495  so this just happened and i am a little taken ...  \n",
       "2497  so , i have been friend with someone for a few...  \n",
       "2498  my first post ! no word were exchanged in this...  \n",
       "2499  background info : i am in college and my dorm ...  \n",
       "\n",
       "[2343 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4fNXEbcJpNCT",
    "outputId": "ce747087-822e-4e53-9f26-ca3f16c88874"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>num_binarized_label</th>\n",
       "      <th>clean</th>\n",
       "      <th>lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>clean1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Backstory: So, I got an Xbox one for Christmas...</td>\n",
       "      <td>backstory: so, i got an xbox one for christmas...</td>\n",
       "      <td>[backstory, :, so, ,, i, got, an, xbox, one, f...</td>\n",
       "      <td>[backstory, :, so, ,, i, got, an, xbox, one, f...</td>\n",
       "      <td>backstory : so , i got an xbox one for christm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>I work with about six other people at might jo...</td>\n",
       "      <td>i work with about six other people at might jo...</td>\n",
       "      <td>[i, work, with, about, six, other, people, at,...</td>\n",
       "      <td>[i, work, with, about, six, other, people, at,...</td>\n",
       "      <td>i work with about six other people at might jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Context: There was an Instagram post about unp...</td>\n",
       "      <td>context: there was an instagram post about unp...</td>\n",
       "      <td>[context, :, there, was, an, instagram, post, ...</td>\n",
       "      <td>[context, :, there, wa, an, instagram, post, a...</td>\n",
       "      <td>context : there wa an instagram post about unp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "      <td>WRONG</td>\n",
       "      <td>1</td>\n",
       "      <td>Me and my friends spent sometime organizing a ...</td>\n",
       "      <td>me and my friends spent sometime organizing a ...</td>\n",
       "      <td>[me, and, my, friends, spent, sometime, organi...</td>\n",
       "      <td>[me, and, my, friend, spent, sometime, organiz...</td>\n",
       "      <td>me and my friend spent sometime organizing a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>A little background. I'm a far from rich guy w...</td>\n",
       "      <td>a little background. i am a far from rich guy ...</td>\n",
       "      <td>[a, little, background, ., i, am, a, far, from...</td>\n",
       "      <td>[a, little, background, ., i, am, a, far, from...</td>\n",
       "      <td>a little background . i am a far from rich guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27761</th>\n",
       "      <td>So a bit of background info. My girlfriend has...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>So a bit of background info. My girlfriend has...</td>\n",
       "      <td>so a bit of background info. my girlfriend has...</td>\n",
       "      <td>[so, a, bit, of, background, info, ., my, girl...</td>\n",
       "      <td>[so, a, bit, of, background, info, ., my, girl...</td>\n",
       "      <td>so a bit of background info . my girlfriend ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27762</th>\n",
       "      <td>Context:\\n\\nI’ve been best friends with this g...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>Context:  I’ve been best friends with this guy...</td>\n",
       "      <td>context:  i have been best friends with this g...</td>\n",
       "      <td>[context, :, i, have, been, best, friends, wit...</td>\n",
       "      <td>[context, :, i, have, been, best, friend, with...</td>\n",
       "      <td>context : i have been best friend with this gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27763</th>\n",
       "      <td>So me (19M) and my gf’s (18F) relationship has...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>So me (19M) and my gf’s (18F) relationship has...</td>\n",
       "      <td>so me (19m) and my gfs (18f) relationship has ...</td>\n",
       "      <td>[so, me, (, 19m, ), and, my, gfs, (, 18f, ), r...</td>\n",
       "      <td>[so, me, (, 19m, ), and, my, gfs, (, 18f, ), r...</td>\n",
       "      <td>so me ( 19m ) and my gfs ( 18f ) relationship ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27764</th>\n",
       "      <td>A little info, I’m an Early College student, f...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>A little info, I’m an Early College student, f...</td>\n",
       "      <td>a little info, i am an early college student, ...</td>\n",
       "      <td>[a, little, info, ,, i, am, an, early, college...</td>\n",
       "      <td>[a, little, info, ,, i, am, an, early, college...</td>\n",
       "      <td>a little info , i am an early college student ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27765</th>\n",
       "      <td>I was on the way home on a long 2 lane bridge ...</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>0</td>\n",
       "      <td>I was on the way home on a long 2 lane bridge ...</td>\n",
       "      <td>i was on the way home on a long 2 lane bridge ...</td>\n",
       "      <td>[i, was, on, the, way, home, on, a, long, 2, l...</td>\n",
       "      <td>[i, wa, on, the, way, home, on, a, long, 2, la...</td>\n",
       "      <td>i wa on the way home on a long 2 lane bridge (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26196 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text binarized_label  \\\n",
       "0      Backstory: So, I got an Xbox one for Christmas...           RIGHT   \n",
       "1      I work with about six other people at might jo...           RIGHT   \n",
       "2      Context: There was an Instagram post about unp...           RIGHT   \n",
       "3      Me and my friends spent sometime organizing a ...           WRONG   \n",
       "4      A little background. I'm a far from rich guy w...           RIGHT   \n",
       "...                                                  ...             ...   \n",
       "27761  So a bit of background info. My girlfriend has...           RIGHT   \n",
       "27762  Context:\\n\\nI’ve been best friends with this g...           RIGHT   \n",
       "27763  So me (19M) and my gf’s (18F) relationship has...           RIGHT   \n",
       "27764  A little info, I’m an Early College student, f...           RIGHT   \n",
       "27765  I was on the way home on a long 2 lane bridge ...           RIGHT   \n",
       "\n",
       "       num_binarized_label                                              clean  \\\n",
       "0                        0  Backstory: So, I got an Xbox one for Christmas...   \n",
       "1                        0  I work with about six other people at might jo...   \n",
       "2                        0  Context: There was an Instagram post about unp...   \n",
       "3                        1  Me and my friends spent sometime organizing a ...   \n",
       "4                        0  A little background. I'm a far from rich guy w...   \n",
       "...                    ...                                                ...   \n",
       "27761                    0  So a bit of background info. My girlfriend has...   \n",
       "27762                    0  Context:  I’ve been best friends with this guy...   \n",
       "27763                    0  So me (19M) and my gf’s (18F) relationship has...   \n",
       "27764                    0  A little info, I’m an Early College student, f...   \n",
       "27765                    0  I was on the way home on a long 2 lane bridge ...   \n",
       "\n",
       "                                                   lower  \\\n",
       "0      backstory: so, i got an xbox one for christmas...   \n",
       "1      i work with about six other people at might jo...   \n",
       "2      context: there was an instagram post about unp...   \n",
       "3      me and my friends spent sometime organizing a ...   \n",
       "4      a little background. i am a far from rich guy ...   \n",
       "...                                                  ...   \n",
       "27761  so a bit of background info. my girlfriend has...   \n",
       "27762  context:  i have been best friends with this g...   \n",
       "27763  so me (19m) and my gfs (18f) relationship has ...   \n",
       "27764  a little info, i am an early college student, ...   \n",
       "27765  i was on the way home on a long 2 lane bridge ...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [backstory, :, so, ,, i, got, an, xbox, one, f...   \n",
       "1      [i, work, with, about, six, other, people, at,...   \n",
       "2      [context, :, there, was, an, instagram, post, ...   \n",
       "3      [me, and, my, friends, spent, sometime, organi...   \n",
       "4      [a, little, background, ., i, am, a, far, from...   \n",
       "...                                                  ...   \n",
       "27761  [so, a, bit, of, background, info, ., my, girl...   \n",
       "27762  [context, :, i, have, been, best, friends, wit...   \n",
       "27763  [so, me, (, 19m, ), and, my, gfs, (, 18f, ), r...   \n",
       "27764  [a, little, info, ,, i, am, an, early, college...   \n",
       "27765  [i, was, on, the, way, home, on, a, long, 2, l...   \n",
       "\n",
       "                                   lemmatize_word_wo_pos  \\\n",
       "0      [backstory, :, so, ,, i, got, an, xbox, one, f...   \n",
       "1      [i, work, with, about, six, other, people, at,...   \n",
       "2      [context, :, there, wa, an, instagram, post, a...   \n",
       "3      [me, and, my, friend, spent, sometime, organiz...   \n",
       "4      [a, little, background, ., i, am, a, far, from...   \n",
       "...                                                  ...   \n",
       "27761  [so, a, bit, of, background, info, ., my, girl...   \n",
       "27762  [context, :, i, have, been, best, friend, with...   \n",
       "27763  [so, me, (, 19m, ), and, my, gfs, (, 18f, ), r...   \n",
       "27764  [a, little, info, ,, i, am, an, early, college...   \n",
       "27765  [i, wa, on, the, way, home, on, a, long, 2, la...   \n",
       "\n",
       "                                                  clean1  \n",
       "0      backstory : so , i got an xbox one for christm...  \n",
       "1      i work with about six other people at might jo...  \n",
       "2      context : there wa an instagram post about unp...  \n",
       "3      me and my friend spent sometime organizing a g...  \n",
       "4      a little background . i am a far from rich guy...  \n",
       "...                                                  ...  \n",
       "27761  so a bit of background info . my girlfriend ha...  \n",
       "27762  context : i have been best friend with this gu...  \n",
       "27763  so me ( 19m ) and my gfs ( 18f ) relationship ...  \n",
       "27764  a little info , i am an early college student ...  \n",
       "27765  i wa on the way home on a long 2 lane bridge (...  \n",
       "\n",
       "[26196 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yTeTguvdFuK"
   },
   "source": [
    "# **Setup train test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6NJxP_LpQNFs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text_transformer = CountVectorizer(input='content',\\n            encoding='utf-8',\\n            decode_error='strict',\\n            preprocessor=None,\\n            tokenizer=None,\\n            token_pattern=r'(?u)\\x08\\\\w\\\\w+\\x08',\\n            max_features=None,\\n            vocabulary=None,\\n            binary=True)\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "text_transformer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "'''text_transformer = CountVectorizer(input='content',\n",
    "            encoding='utf-8',\n",
    "            decode_error='strict',\n",
    "            preprocessor=None,\n",
    "            tokenizer=None,\n",
    "            token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "            max_features=None,\n",
    "            vocabulary=None,\n",
    "            binary=True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "75SjyfmJiZhJ"
   },
   "outputs": [],
   "source": [
    "train_x = train_df[[\"clean1\"]]\n",
    "train_y = train_df[[\"num_binarized_label\"]]\n",
    "\n",
    "dev_x = dev_df[[\"clean1\"]]\n",
    "dev_y = dev_df[[\"num_binarized_label\"]]\n",
    "\n",
    "test_x = test_df[[\"clean1\"]]\n",
    "test_y = test_df[[\"num_binarized_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CbvwCwpjiZWY"
   },
   "outputs": [],
   "source": [
    "train_x_text = text_transformer.fit_transform(train_x['clean1'])\n",
    "dev_x_text = text_transformer.transform(dev_x['clean1'])\n",
    "test_x_text = text_transformer.transform(test_x['clean1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "2UTjl8fle6K-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X = train_x_text.toarray()'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''X = train_x_text.toarray()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpZkdPutiZTJ",
    "outputId": "68761bc7-b653-4f5d-f94d-6f0238642e2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26196, 42646)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hufN_riVsKB"
   },
   "source": [
    "# **GaussianNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nhpPGze4jW-9",
    "outputId": "731a21cf-bb91-46bd-d87e-f49f64f84a43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model = GaussianNB()\\n\\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\\ngs_NB = GridSearchCV(estimator=model, \\n                 param_grid=params_NB, \\n                 cv=5,   # use any cross validation technique \\n                 verbose=1, \\n                 scoring='accuracy')\\n\\ngs_NB.fit(X, train_y)\\n\\ngs_NB.best_params_\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#https://stackoverflow.com/questions/39828535/how-to-tune-gaussiannb\n",
    "model = GaussianNB()\n",
    "model.fit(train_x_text.toarray(), train_y)\n",
    "\n",
    "'''model = GaussianNB()\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=model, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=5,   # use any cross validation technique \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy')\n",
    "\n",
    "gs_NB.fit(X, train_y)\n",
    "\n",
    "gs_NB.best_params_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgLnzoebRWcG",
    "outputId": "7b2e9025-03ea-4cd2-97df-0f341ee5f8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[880 947]\n",
      " [256 275]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.48      0.59      1827\n",
      "           1       0.23      0.52      0.31       531\n",
      "\n",
      "    accuracy                           0.49      2358\n",
      "   macro avg       0.50      0.50      0.45      2358\n",
      "weighted avg       0.65      0.49      0.53      2358\n",
      "\n",
      "0.48982188295165396\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(test_x_text.toarray())\n",
    "\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "print(classification_report(test_y,test_pred))\n",
    "print(accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "atrM8RJWSRcK",
    "outputId": "3dc00c63-960d-4748-9aa8-33dede7fe2b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>clean</th>\n",
       "      <th>lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>clean1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_binarized_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  binarized_label  clean  lower  tokenized  \\\n",
       "num_binarized_label                                                   \n",
       "0                    1827             1827   1827   1827       1827   \n",
       "1                     531              531    531    531        531   \n",
       "\n",
       "                     lemmatize_word_wo_pos  clean1  \n",
       "num_binarized_label                                 \n",
       "0                                     1827    1827  \n",
       "1                                      531     531  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('num_binarized_label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDyR5PFXVt4_"
   },
   "source": [
    "# **MultinomialNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tNNBtW8VttC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "DblDl8f-Vzsw",
    "outputId": "6bae4284-24e2-457f-bc51-ed8efc2f81bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#https://stackoverflow.com/questions/39828535/how-to-tune-gaussiannb\n",
    "model = MultinomialNB()\n",
    "model.fit(train_x_text.toarray(), train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kvh1oyt7Vzs1",
    "outputId": "056e8adf-c308-46a1-f5c0-f575dabe31b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1827    0]\n",
      " [ 531    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87      1827\n",
      "           1       0.00      0.00      0.00       531\n",
      "\n",
      "    accuracy                           0.77      2358\n",
      "   macro avg       0.39      0.50      0.44      2358\n",
      "weighted avg       0.60      0.77      0.68      2358\n",
      "\n",
      "0.7748091603053435\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(test_x_text.toarray())\n",
    "\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "print(classification_report(test_y,test_pred))\n",
    "print(accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "bHtnaWI-Vzs1",
    "outputId": "137c7c1b-2f16-4e5a-e1da-352dd266130b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binarized_label</th>\n",
       "      <th>clean</th>\n",
       "      <th>lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>clean1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_binarized_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  binarized_label  clean  lower  tokenized  \\\n",
       "num_binarized_label                                                   \n",
       "0                    1827             1827   1827   1827       1827   \n",
       "1                     531              531    531    531        531   \n",
       "\n",
       "                     lemmatize_word_wo_pos  clean1  \n",
       "num_binarized_label                                 \n",
       "0                                     1827    1827  \n",
       "1                                      531     531  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('num_binarized_label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iibLSvrIa4nu",
    "6inA2y46ayU3",
    "_hufN_riVsKB"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
